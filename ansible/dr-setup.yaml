---
# =============================================================================
# NetApp DR Starter Kit - Main Playbook
# =============================================================================
#
# This playbook orchestrates the complete DR infrastructure setup:
#   0. Reads values-trident.yaml for the terraform state bucket name
#   1. Discovers prod and DR cluster info (names, regions, VPC CIDRs)
#   2. Creates S3 bucket + DynamoDB table for Terraform remote state
#   3. Peers the VPCs of the two clusters
#   4. Creates FSx for NetApp ONTAP filesystems in each region
#
# The FSx filesystems are named: <cluster_name>-<region>-fsx
# and are intended for use with Trident / Trident Protect for DR replication.
#
# Usage:
#   ansible-playbook ansible/dr-setup.yaml \
#     -e @ansible/dr-vars.yml \
#     -e prod_kubeconfig=/path/to/prod/kubeconfig \
#     -e dr_kubeconfig=/path/to/dr/kubeconfig
#
# =============================================================================

# -----------------------------------------------------------------------------
# Play 1: Load values-trident.yaml and Discover Production Cluster
# -----------------------------------------------------------------------------
- name: Discover Production Cluster
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Load values-trident.yaml
      set_fact:
        _values_trident: "{{ lookup('file', playbook_dir ~ '/../values-trident.yaml') | from_yaml }}"

    - name: Set terraform state bucket from values-trident.yaml
      set_fact:
        terraform_state_bucket: "{{ terraform_state_bucket | default(_values_trident.terraform.state.bucket) }}"
        cacheable: true

    - name: Display state bucket
      debug:
        msg: "Terraform state bucket (from values-trident.yaml): {{ terraform_state_bucket }}"

    - name: Validate prod_kubeconfig is provided
      fail:
        msg: "prod_kubeconfig is required. Pass it via -e prod_kubeconfig=/path/to/kubeconfig"
      when: prod_kubeconfig is not defined or prod_kubeconfig == ''

    - name: Discover production cluster
      include_role:
        name: cluster_discovery
      vars:
        kubeconfig: "{{ prod_kubeconfig }}"
        cluster_label: "Production"
        cluster_name_override: "{{ prod_cluster_name_override | default('') }}"
        cluster_region_override: "{{ prod_region_override | default('') }}"
        cluster_vpc_cidr_override: "{{ prod_vpc_cidr_override | default('') }}"
        cluster_vpc_name_override: "{{ prod_vpc_name_override | default('') }}"

    - name: Store production cluster info
      set_fact:
        prod_cluster: "{{ discovered_cluster }}"
        cacheable: true

# -----------------------------------------------------------------------------
# Play 2: Discover DR Cluster
# -----------------------------------------------------------------------------
- name: Discover DR Cluster
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Validate dr_kubeconfig is provided
      fail:
        msg: "dr_kubeconfig is required. Pass it via -e dr_kubeconfig=/path/to/kubeconfig"
      when: dr_kubeconfig is not defined or dr_kubeconfig == ''

    - name: Discover DR cluster
      include_role:
        name: cluster_discovery
      vars:
        kubeconfig: "{{ dr_kubeconfig }}"
        cluster_label: "DR"
        cluster_name_override: "{{ dr_cluster_name_override | default('') }}"
        cluster_region_override: "{{ dr_region_override | default('') }}"
        cluster_vpc_cidr_override: "{{ dr_vpc_cidr_override | default('') }}"
        cluster_vpc_name_override: "{{ dr_vpc_name_override | default('') }}"

    - name: Store DR cluster info
      set_fact:
        dr_cluster: "{{ discovered_cluster }}"
        cacheable: true

# -----------------------------------------------------------------------------
# Play 3: Bootstrap Terraform State Backend (S3 + DynamoDB)
#   Only runs on create — destroy is handled separately via
#   "make destroy-terraform-state" to prevent accidental deletion.
# -----------------------------------------------------------------------------
- name: Create Terraform State Backend
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip state backend creation if destroying
      meta: end_play
      when: destroy_resources | default(false)

    - name: Create S3 bucket and DynamoDB table for Terraform state
      include_role:
        name: terraform_state
      vars:
        terraform_state_region: "{{ prod_cluster.region }}"

# -----------------------------------------------------------------------------
# Play 4: VPC Peering (create only — destroy is in the destroy sequence)
# -----------------------------------------------------------------------------
- name: Peer VPCs between Production and DR
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip VPC peering if destroying
      meta: end_play
      when: destroy_resources | default(false)

    - name: Display peering summary
      debug:
        msg:
          - "=========================================="
          - "VPC Peering Setup"
          - "=========================================="
          - "Prod: {{ prod_cluster.cluster_name }} in {{ prod_cluster.region }} ({{ prod_cluster.vpc_cidr }})"
          - "DR:   {{ dr_cluster.cluster_name }} in {{ dr_cluster.region }} ({{ dr_cluster.vpc_cidr }})"
          - "=========================================="

    - name: Peer VPCs via Terraform
      include_role:
        name: vpc_peering_terraform

# -----------------------------------------------------------------------------
# Play 5: Create FSx ONTAP in Both Regions (parallel)
#
# FSx creation takes 20-40 minutes. Both filesystems are independent, so
# we prepare both (setup, init, plan) sequentially, then fire both applies
# in parallel using async, cutting total wall-clock time roughly in half.
# -----------------------------------------------------------------------------
- name: Create FSx ONTAP Filesystems (parallel)
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip FSx creation if destroy is requested
      meta: end_play
      when: destroy_resources | default(false)

    # --- Load FSx password ---
    - name: Load FSx password from ~/.fsx file
      block:
        - name: Check if ~/.fsx file exists
          stat:
            path: "{{ '~/.fsx' | expanduser }}"
          register: _fsx_file_stat

        - name: Read password from ~/.fsx file
          slurp:
            src: "{{ '~/.fsx' | expanduser }}"
          register: _fsx_file_content
          when: _fsx_file_stat.stat.exists | default(false)

        - name: Set password from ~/.fsx file
          set_fact:
            fsx_admin_password: "{{ _fsx_file_content.content | b64decode | trim }}"
            svm_admin_password: "{{ _fsx_file_content.content | b64decode | trim }}"
          when:
            - _fsx_file_stat.stat.exists | default(false)
            - _fsx_file_content.content is defined
            - fsx_admin_password is not defined or fsx_admin_password == ''
      when: fsx_admin_password is not defined or fsx_admin_password == ''

    - name: Validate FSx password is set
      fail:
        msg: |
          fsx_admin_password is required but not found.
          Please either:
            1. Create ~/.fsx file with the password: echo "MyPassword" > ~/.fsx && chmod 600 ~/.fsx
            2. Pass via command line: -e fsx_admin_password=MyPassword
      when: fsx_admin_password is not defined or fsx_admin_password == ''

    # --- Prepare Production FSx (setup, init, plan) ---
    - name: Prepare Production FSx ONTAP
      include_role:
        name: fsx_ontap_terraform
      vars:
        cluster_info: "{{ prod_cluster }}"
        fsx_label: "Production FSx"
        file_system_name_override: "{{ prod_file_system_name | default('') }}"
        terraform_state_region: "{{ prod_cluster.region }}"
        svm_name: "{{ prod_cluster.cluster_name }}-SVM1"
        allowed_cidrs:
          - "{{ prod_cluster.vpc_cidr }}"
          - "{{ dr_cluster.vpc_cidr }}"
        fsx_prepare_only: true

    - name: Save Production FSx prepared state
      set_fact:
        _prod_fsx_prepared: "{{ fsx_prepared }}"

    # --- Prepare DR FSx (setup, init, plan) ---
    - name: Prepare DR FSx ONTAP
      include_role:
        name: fsx_ontap_terraform
      vars:
        cluster_info: "{{ dr_cluster }}"
        fsx_label: "DR FSx"
        file_system_name_override: "{{ dr_file_system_name | default('') }}"
        terraform_state_region: "{{ prod_cluster.region }}"
        svm_name: "{{ dr_cluster.cluster_name }}-SVM1"
        allowed_cidrs:
          - "{{ dr_cluster.vpc_cidr }}"
          - "{{ prod_cluster.vpc_cidr }}"
        fsx_prepare_only: true

    - name: Save DR FSx prepared state
      set_fact:
        _dr_fsx_prepared: "{{ fsx_prepared }}"

    # --- Apply both in parallel ---
    - name: Display parallel apply notice
      debug:
        msg:
          - "=========================================="
          - "Applying FSx ONTAP in PARALLEL"
          - "=========================================="
          - "  Production: {{ _prod_fsx_prepared.file_system_name }}"
          - "  DR:         {{ _dr_fsx_prepared.file_system_name }}"
          - "  This may take 20-40 minutes..."
          - "=========================================="

    - name: Apply FSx ONTAP - Production (async)
      ansible.builtin.shell: "{{ _prod_fsx_prepared.apply_cmd }}"
      async: 3600
      poll: 0
      register: _prod_fsx_job
      when: not (_prod_fsx_prepared.skipped | default(false))

    - name: Apply FSx ONTAP - DR (async)
      ansible.builtin.shell: "{{ _dr_fsx_prepared.apply_cmd }}"
      async: 3600
      poll: 0
      register: _dr_fsx_job
      when: not (_dr_fsx_prepared.skipped | default(false))

    # --- Wait for both to complete ---
    - name: Wait for Production FSx apply to complete
      ansible.builtin.async_status:
        jid: "{{ _prod_fsx_job.ansible_job_id }}"
      register: _prod_fsx_apply
      until: _prod_fsx_apply.finished
      retries: 120
      delay: 30
      when: not (_prod_fsx_prepared.skipped | default(false))

    - name: Wait for DR FSx apply to complete
      ansible.builtin.async_status:
        jid: "{{ _dr_fsx_job.ansible_job_id }}"
      register: _dr_fsx_apply
      until: _dr_fsx_apply.finished
      retries: 120
      delay: 30
      when: not (_dr_fsx_prepared.skipped | default(false))

    # --- Get outputs ---
    - name: Get Production FSx outputs
      ansible.builtin.shell: "{{ _prod_fsx_prepared.output_cmd }}"
      register: _prod_fsx_outputs_raw
      changed_when: false
      when: not (_prod_fsx_prepared.skipped | default(false))

    - name: Get DR FSx outputs
      ansible.builtin.shell: "{{ _dr_fsx_prepared.output_cmd }}"
      register: _dr_fsx_outputs_raw
      changed_when: false
      when: not (_dr_fsx_prepared.skipped | default(false))

    # --- Store results ---
    - name: Store Production FSx result
      set_fact:
        prod_fsx:
          file_system_id: "{{ (_prod_fsx_outputs_raw.stdout | from_json).file_system_id.value }}"
          file_system_dns_name: "{{ (_prod_fsx_outputs_raw.stdout | from_json).file_system_dns_name.value }}"
          mgmt_ips: "{{ (_prod_fsx_outputs_raw.stdout | from_json).management_endpoint_ip_addresses.value | default([]) }}"
          svm_id: "{{ (_prod_fsx_outputs_raw.stdout | from_json).svm_id.value }}"
          svm_name: "{{ (_prod_fsx_outputs_raw.stdout | from_json).svm_name.value }}"
          svm_mgmt_dns_name: "{{ (_prod_fsx_outputs_raw.stdout | from_json).svm_management_endpoint_dns_name.value | default('') }}"
          svm_mgmt_ips: "{{ (_prod_fsx_outputs_raw.stdout | from_json).svm_management_endpoint_ip_addresses.value | default([]) }}"
          intercluster_dns_name: "{{ (_prod_fsx_outputs_raw.stdout | from_json).intercluster_endpoint_dns_name.value | default('') }}"
          intercluster_ips: "{{ (_prod_fsx_outputs_raw.stdout | from_json).intercluster_endpoint_ip_addresses.value | default([]) }}"
          security_group_id: "{{ (_prod_fsx_outputs_raw.stdout | from_json).security_group_id.value }}"
          file_system_name: "{{ _prod_fsx_prepared.file_system_name }}"
        cacheable: true
      when: not (_prod_fsx_prepared.skipped | default(false))

    - name: Store DR FSx result
      set_fact:
        dr_fsx:
          file_system_id: "{{ (_dr_fsx_outputs_raw.stdout | from_json).file_system_id.value }}"
          file_system_dns_name: "{{ (_dr_fsx_outputs_raw.stdout | from_json).file_system_dns_name.value }}"
          mgmt_ips: "{{ (_dr_fsx_outputs_raw.stdout | from_json).management_endpoint_ip_addresses.value | default([]) }}"
          svm_id: "{{ (_dr_fsx_outputs_raw.stdout | from_json).svm_id.value }}"
          svm_name: "{{ (_dr_fsx_outputs_raw.stdout | from_json).svm_name.value }}"
          svm_mgmt_dns_name: "{{ (_dr_fsx_outputs_raw.stdout | from_json).svm_management_endpoint_dns_name.value | default('') }}"
          svm_mgmt_ips: "{{ (_dr_fsx_outputs_raw.stdout | from_json).svm_management_endpoint_ip_addresses.value | default([]) }}"
          intercluster_dns_name: "{{ (_dr_fsx_outputs_raw.stdout | from_json).intercluster_endpoint_dns_name.value | default('') }}"
          intercluster_ips: "{{ (_dr_fsx_outputs_raw.stdout | from_json).intercluster_endpoint_ip_addresses.value | default([]) }}"
          security_group_id: "{{ (_dr_fsx_outputs_raw.stdout | from_json).security_group_id.value }}"
          file_system_name: "{{ _dr_fsx_prepared.file_system_name }}"
        cacheable: true
      when: not (_dr_fsx_prepared.skipped | default(false))

# -----------------------------------------------------------------------------
# Play 7: Update values files with FSx management LIF IPs
# -----------------------------------------------------------------------------
- name: Update values files with FSx mgmtLIF
  hosts: localhost
  gather_facts: false
  vars:
    _project_root: "{{ playbook_dir }}/.."
  tasks:
    - name: Skip values update if destroying
      meta: end_play
      when: destroy_resources | default(false)

    - name: Set mgmtLIF and clusterLIF IPs from FSx outputs
      set_fact:
        _hub_mgmt_lif: "{{ prod_fsx.svm_mgmt_ips | first }}"
        _hub_cluster_lif: "{{ prod_fsx.mgmt_ips | first }}"
        _secondary_mgmt_lif: "{{ dr_fsx.svm_mgmt_ips | first }}"
        _secondary_cluster_lif: "{{ dr_fsx.mgmt_ips | first }}"

    - name: Update values-hub.yaml .tridentFSX
      ansible.builtin.shell: |
        yq -i '.tridentFSX.mgmtLIF = "{{ _hub_mgmt_lif }}"' {{ _project_root }}/values-hub.yaml
        yq -i '.tridentFSX.clusterLIF = "{{ _hub_cluster_lif }}"' {{ _project_root }}/values-hub.yaml
        yq -i '.tridentFSX.svm = "{{ prod_fsx.svm_name }}"' {{ _project_root }}/values-hub.yaml
      changed_when: true

    - name: Update values-secondary.yaml .tridentFSX
      ansible.builtin.shell: |
        yq -i '.tridentFSX.mgmtLIF = "{{ _secondary_mgmt_lif }}"' {{ _project_root }}/values-secondary.yaml
        yq -i '.tridentFSX.clusterLIF = "{{ _secondary_cluster_lif }}"' {{ _project_root }}/values-secondary.yaml
        yq -i '.tridentFSX.svm = "{{ dr_fsx.svm_name }}"' {{ _project_root }}/values-secondary.yaml
      changed_when: true

    - name: Update values-hub.yaml intercluster and peer info
      ansible.builtin.shell: |
        yq -i '.tridentFSX.interclusterIPs = {{ prod_fsx.intercluster_ips | to_json }}' {{ _project_root }}/values-hub.yaml
        yq -i '.tridentFSX.peer.mgmtLIF = "{{ _secondary_mgmt_lif }}"' {{ _project_root }}/values-hub.yaml
        yq -i '.tridentFSX.peer.clusterLIF = "{{ _secondary_cluster_lif }}"' {{ _project_root }}/values-hub.yaml
        yq -i '.tridentFSX.peer.svm = "{{ dr_fsx.svm_name }}"' {{ _project_root }}/values-hub.yaml
        yq -i '.tridentFSX.peer.interclusterIPs = {{ dr_fsx.intercluster_ips | to_json }}' {{ _project_root }}/values-hub.yaml
      changed_when: true

    - name: Update values-secondary.yaml intercluster and peer info
      ansible.builtin.shell: |
        yq -i '.tridentFSX.interclusterIPs = {{ dr_fsx.intercluster_ips | to_json }}' {{ _project_root }}/values-secondary.yaml
        yq -i '.tridentFSX.peer.mgmtLIF = "{{ _hub_mgmt_lif }}"' {{ _project_root }}/values-secondary.yaml
        yq -i '.tridentFSX.peer.clusterLIF = "{{ _hub_cluster_lif }}"' {{ _project_root }}/values-secondary.yaml
        yq -i '.tridentFSX.peer.svm = "{{ prod_fsx.svm_name }}"' {{ _project_root }}/values-secondary.yaml
        yq -i '.tridentFSX.peer.interclusterIPs = {{ prod_fsx.intercluster_ips | to_json }}' {{ _project_root }}/values-secondary.yaml
      changed_when: true

    - name: Display values file updates
      debug:
        msg:
          - "=========================================="
          - "Values Files Updated"
          - "=========================================="
          - "  values-hub.yaml:"
          - "    tridentFSX.mgmtLIF:            {{ _hub_mgmt_lif }}"
          - "    tridentFSX.clusterLIF:         {{ _hub_cluster_lif }}"
          - "    tridentFSX.svm:                {{ prod_fsx.svm_name }}"
          - "    tridentFSX.interclusterIPs:    {{ prod_fsx.intercluster_ips }}"
          - "    tridentFSX.peer.mgmtLIF:       {{ _secondary_mgmt_lif }}"
          - "    tridentFSX.peer.clusterLIF:    {{ _secondary_cluster_lif }}"
          - "    tridentFSX.peer.svm:            {{ dr_fsx.svm_name }}"
          - "    tridentFSX.peer.interclusterIPs: {{ dr_fsx.intercluster_ips }}"
          - ""
          - "  values-secondary.yaml:"
          - "    tridentFSX.mgmtLIF:            {{ _secondary_mgmt_lif }}"
          - "    tridentFSX.clusterLIF:         {{ _secondary_cluster_lif }}"
          - "    tridentFSX.svm:                {{ dr_fsx.svm_name }}"
          - "    tridentFSX.interclusterIPs:    {{ dr_fsx.intercluster_ips }}"
          - "    tridentFSX.peer.mgmtLIF:       {{ _hub_mgmt_lif }}"
          - "    tridentFSX.peer.clusterLIF:    {{ _hub_cluster_lif }}"
          - "    tridentFSX.peer.svm:            {{ prod_fsx.svm_name }}"
          - "    tridentFSX.peer.interclusterIPs: {{ prod_fsx.intercluster_ips }}"
          - "=========================================="

# -----------------------------------------------------------------------------
# Play 8: Summary
# -----------------------------------------------------------------------------
- name: DR Infrastructure Summary
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip summary if destroy was requested
      meta: end_play
      when: destroy_resources | default(false)

    - name: Display complete DR infrastructure summary
      debug:
        msg:
          - "============================================================"
          - "  NetApp DR Infrastructure - Setup Complete"
          - "============================================================"
          - ""
          - "  TERRAFORM STATE"
          - "    S3 Bucket:   {{ terraform_state_bucket }}"
          - "    DynamoDB:    {{ terraform_state_dynamodb_table | default('terraform-state-lock') }}"
          - ""
          - "  PRODUCTION CLUSTER"
          - "    Cluster:     {{ prod_cluster.cluster_name }}"
          - "    Region:      {{ prod_cluster.region }}"
          - "    VPC:         {{ prod_cluster.vpc_id }} ({{ prod_cluster.vpc_cidr }})"
          - "    FSx Name:    {{ prod_fsx.file_system_name }}"
          - "    FSx ID:      {{ prod_fsx.file_system_id }}"
          - "    SVM Mgmt:    {{ prod_fsx.svm_mgmt_dns_name }}"
          - "    Intercluster: {{ prod_fsx.intercluster_dns_name }}"
          - ""
          - "  DR CLUSTER"
          - "    Cluster:     {{ dr_cluster.cluster_name }}"
          - "    Region:      {{ dr_cluster.region }}"
          - "    VPC:         {{ dr_cluster.vpc_id }} ({{ dr_cluster.vpc_cidr }})"
          - "    FSx Name:    {{ dr_fsx.file_system_name }}"
          - "    FSx ID:      {{ dr_fsx.file_system_id }}"
          - "    SVM Mgmt:    {{ dr_fsx.svm_mgmt_dns_name }}"
          - "    Intercluster: {{ dr_fsx.intercluster_dns_name }}"
          - ""
          - "  NEXT STEPS"
          - "    1. Configure Trident backends on both clusters using"
          - "       the SVM management DNS names above"
          - "    2. Configure Trident Protect AppMirrorRelationships"
          - "       for cross-region DR replication"
          - "    3. The FSx intercluster endpoints above are used by"
          - "       SnapMirror for data replication between the"
          - "       ONTAP filesystems"
          - "============================================================"

# -----------------------------------------------------------------------------
# Destroy Plays (only run when destroy_resources=true)
#
# Order matters — reverse of creation:
#   0. Remove ONTAP SVM/cluster peering (required before SVM deletion)
#   1. FSx filesystems (both regions)
#   2. VPC peering
#
# Note: The Terraform state backend (S3 + DynamoDB) is NOT destroyed here.
# Use "make destroy-terraform-state" separately if you want to remove it.
# -----------------------------------------------------------------------------
- name: Remove ONTAP Peering Relationships
  hosts: localhost
  gather_facts: false
  vars:
    _project_root: "{{ playbook_dir }}/.."
    _prod_fsx_name: "{{ prod_file_system_name | default(prod_cluster.cluster_name ~ '-' ~ prod_cluster.region ~ '-fsx') }}"
    _dr_fsx_name: "{{ dr_file_system_name | default(dr_cluster.cluster_name ~ '-' ~ dr_cluster.region ~ '-fsx') }}"
  tasks:
    - name: Skip if not destroying
      meta: end_play
      when: not (destroy_resources | default(false))

    - name: Load FSx password from ~/.fsx file
      block:
        - name: Check if ~/.fsx file exists
          stat:
            path: "{{ '~/.fsx' | expanduser }}"
          register: _fsx_file_stat

        - name: Read password from ~/.fsx file
          slurp:
            src: "{{ '~/.fsx' | expanduser }}"
          register: _fsx_file_content
          when: _fsx_file_stat.stat.exists | default(false)

        - name: Set password from ~/.fsx file
          set_fact:
            fsx_admin_password: "{{ _fsx_file_content.content | b64decode | trim }}"
          when:
            - _fsx_file_stat.stat.exists | default(false)
            - _fsx_file_content.content is defined
            - fsx_admin_password is not defined or fsx_admin_password == ''
      when: fsx_admin_password is not defined or fsx_admin_password == ''

    - name: Read Production cluster management LIF
      ansible.builtin.shell: >-
        yq '.tridentFSX.clusterLIF // ""' {{ _project_root }}/values-hub.yaml
      register: _prod_lif_raw
      changed_when: false
      failed_when: false

    - name: Read DR cluster management LIF
      ansible.builtin.shell: >-
        yq '.tridentFSX.clusterLIF // ""' {{ _project_root }}/values-secondary.yaml
      register: _dr_lif_raw
      changed_when: false
      failed_when: false

    - name: Set cluster LIF facts
      set_fact:
        _prod_cluster_lif: "{{ _prod_lif_raw.stdout | default('') | trim }}"
        _dr_cluster_lif: "{{ _dr_lif_raw.stdout | default('') | trim }}"

    - name: Display peering teardown info
      debug:
        msg:
          - "=========================================="
          - "Removing ONTAP Peering Before Destroy"
          - "=========================================="
          - "  Production clusterLIF: {{ _prod_cluster_lif }}"
          - "  DR clusterLIF:         {{ _dr_cluster_lif }}"
          - "  Production FSx SG:     {{ _prod_fsx_name }}-sg"
          - "  DR FSx SG:             {{ _dr_fsx_name }}-sg"
          - "=========================================="

    # ------------------------------------------------------------------
    # Ensure FSx security groups have the minimum rules needed for
    # ONTAP management API access and intercluster communication.
    # A prior partial Terraform destroy may have removed these rules,
    # leaving the cluster peers unable to communicate and blocking
    # SVM peer deletion (which deadlocks SVM/filesystem destroy).
    # ------------------------------------------------------------------
    - name: Ensure FSx security group connectivity rules
      ansible.builtin.shell: |
        SG_NAME="{{ item.sg_name }}"
        REGION="{{ item.region }}"

        SG_ID=$(aws ec2 describe-security-groups --region ${REGION} \
          --filters "Name=group-name,Values=${SG_NAME}" \
          --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null)

        if [ -z "${SG_ID}" ] || [ "${SG_ID}" = "None" ]; then
          echo "SKIP: Security group ${SG_NAME} not found in ${REGION}"
          exit 0
        fi

        echo "SG_ID=${SG_ID}"
        ADDED=""

        aws ec2 authorize-security-group-egress --region ${REGION} --group-id ${SG_ID} \
          --ip-permissions 'IpProtocol=-1,IpRanges=[{CidrIp=0.0.0.0/0}]' 2>/dev/null && ADDED="${ADDED} egress-all"

        for CIDR in "{{ prod_cluster.vpc_cidr }}" "{{ dr_cluster.vpc_cidr }}"; do
          for PORT in 443 11104 11105; do
            aws ec2 authorize-security-group-ingress --region ${REGION} --group-id ${SG_ID} \
              --protocol tcp --port ${PORT} --cidr ${CIDR} 2>/dev/null && ADDED="${ADDED} ${PORT}/${CIDR}"
          done
        done

        if [ -n "${ADDED}" ]; then
          echo "Restored missing rules:${ADDED}"
        else
          echo "All required connectivity rules already present"
        fi
      loop:
        - sg_name: "{{ _prod_fsx_name }}-sg"
          region: "{{ prod_cluster.region }}"
        - sg_name: "{{ _dr_fsx_name }}-sg"
          region: "{{ dr_cluster.region }}"
      loop_control:
        label: "{{ item.sg_name }}"
      register: _sg_ensure
      changed_when: "'Restored missing rules' in (item.stdout | default(''))"
      failed_when: false

    - name: Display SG rule restoration results
      debug:
        msg: "{{ item.stdout_lines | default(['(skipped)']) }}"
      loop: "{{ _sg_ensure.results }}"
      loop_control:
        label: "{{ item.item.sg_name }}"
      when: item.stdout is defined

    - name: Wait for restored security group rules to take effect
      pause:
        seconds: 10
      when: _sg_ensure.changed | default(false)

    # ------------------------------------------------------------------
    # Tear down ONTAP peering on each side. The script runs inside the
    # cluster (via oc run) so it can reach the FSx management LIFs
    # which are only accessible from within the VPC.
    # ------------------------------------------------------------------
    - name: Write ONTAP unpeer script
      ansible.builtin.copy:
        dest: /tmp/ontap-unpeer.sh
        mode: "0755"
        content: |
          #!/bin/bash
          MGMT="${ONTAP_MGMT}"
          AUTH="${ONTAP_AUTH}"

          echo "=== Testing connectivity to ${MGMT} ==="
          HTTP_CODE=$(curl -sk -o /dev/null -w '%{http_code}' --connect-timeout 10 \
            -u "${AUTH}" "https://${MGMT}/api/cluster" 2>/dev/null || echo "000")
          if [ "${HTTP_CODE}" = "000" ]; then
            echo "WARNING: Cannot reach FSx management at ${MGMT}"
            echo "Peering teardown skipped"
            exit 0
          fi
          echo "Connected (HTTP ${HTTP_CODE})"

          CL_COUNT=$(curl -sk -u "${AUTH}" "https://${MGMT}/api/cluster/peers" 2>/dev/null \
            | jq -r '.num_records // 0')
          if [ "${CL_COUNT}" = "0" ]; then
            echo "No cluster peers found — nothing to do"
            exit 0
          fi

          echo "=== Waiting for cluster peer connectivity ==="
          for i in $(seq 1 24); do
            STATE=$(curl -sk -u "${AUTH}" \
              "https://${MGMT}/api/cluster/peers?fields=status" 2>/dev/null \
              | jq -r '.records[0].status.state // "unknown"')
            if [ "${STATE}" = "available" ]; then
              echo "  Cluster peer is available"
              break
            fi
            echo "  Cluster peer state: ${STATE} (${i}/24, waiting 10s)..."
            sleep 10
          done

          echo "=== Deleting SVM peers ==="
          for UUID in $(curl -sk -u "${AUTH}" \
            "https://${MGMT}/api/svm/peers?fields=uuid" 2>/dev/null \
            | jq -r '.records[].uuid // empty'); do
            echo "  DELETE SVM peer ${UUID}..."
            curl -sk -u "${AUTH}" -X DELETE \
              "https://${MGMT}/api/svm/peers/${UUID}" 2>/dev/null | jq -c '.' 2>/dev/null
          done

          echo "=== Polling SVM peer removal (max 2 min) ==="
          for i in $(seq 1 24); do
            COUNT=$(curl -sk -u "${AUTH}" "https://${MGMT}/api/svm/peers" 2>/dev/null \
              | jq -r '.num_records // 0')
            if [ "${COUNT}" = "0" ]; then
              echo "  All SVM peers removed"
              break
            fi
            echo "  ${COUNT} SVM peer(s) remaining (${i}/24)..."
            sleep 5
          done

          echo "=== Deleting cluster peers ==="
          for ATTEMPT in 1 2 3; do
            REMAINING=0
            for UUID in $(curl -sk -u "${AUTH}" \
              "https://${MGMT}/api/cluster/peers?fields=uuid" 2>/dev/null \
              | jq -r '.records[].uuid // empty'); do
              echo "  DELETE cluster peer ${UUID} (attempt ${ATTEMPT})..."
              RESP=$(curl -sk -u "${AUTH}" -X DELETE \
                "https://${MGMT}/api/cluster/peers/${UUID}" -w '\n' 2>/dev/null)
              if echo "${RESP}" | jq -e '.error' >/dev/null 2>&1; then
                echo "  Error: $(echo "${RESP}" | jq -r '.error.message')"
                REMAINING=$((REMAINING + 1))
              fi
            done
            [ ${REMAINING} -eq 0 ] && break
            echo "  Retrying in 10s..."
            sleep 10
          done

          SVM_LEFT=$(curl -sk -u "${AUTH}" "https://${MGMT}/api/svm/peers" 2>/dev/null \
            | jq -r '.num_records // 0')
          CL_LEFT=$(curl -sk -u "${AUTH}" "https://${MGMT}/api/cluster/peers" 2>/dev/null \
            | jq -r '.num_records // 0')
          echo "=== Done: ${SVM_LEFT} SVM peer(s), ${CL_LEFT} cluster peer(s) remaining ==="

    - name: Remove ONTAP peering from Production FSx (running in-cluster pod)
      ansible.builtin.shell: |
        B64=$(base64 -w0 /tmp/ontap-unpeer.sh)
        echo ">>> Launching unpeer pod on Production cluster ({{ _prod_cluster_lif }})..."
        echo ">>> This may take 1-5 minutes if peers need to be removed."
        KUBECONFIG="{{ prod_kubeconfig }}" oc run ontap-unpeer-prod \
          --rm --restart=Never --attach=true \
          --image=image-registry.openshift-image-registry.svc:5000/openshift/tools \
          --env="ONTAP_MGMT={{ _prod_cluster_lif }}" \
          --env="ONTAP_AUTH=fsxadmin:{{ fsx_admin_password }}" \
          --env="SCRIPT_B64=${B64}" \
          --overrides='{"spec":{"terminationGracePeriodSeconds":0,"activeDeadlineSeconds":300}}' \
          -- bash -c 'echo ${SCRIPT_B64} | base64 -d | bash'
        RC=$?
        echo ">>> Production unpeer pod exited (rc=${RC})"
      register: _prod_unpeer
      changed_when: true
      failed_when: false
      when: _prod_cluster_lif | default('') != ''

    - name: Display Production unpeer output
      debug:
        var: _prod_unpeer.stdout_lines
      when: _prod_unpeer is not skipped

    - name: Remove ONTAP peering from DR FSx (running in-cluster pod)
      ansible.builtin.shell: |
        B64=$(base64 -w0 /tmp/ontap-unpeer.sh)
        echo ">>> Launching unpeer pod on DR cluster ({{ _dr_cluster_lif }})..."
        echo ">>> This may take 1-5 minutes if peers need to be removed."
        KUBECONFIG="{{ dr_kubeconfig }}" oc run ontap-unpeer-dr \
          --rm --restart=Never --attach=true \
          --image=image-registry.openshift-image-registry.svc:5000/openshift/tools \
          --env="ONTAP_MGMT={{ _dr_cluster_lif }}" \
          --env="ONTAP_AUTH=fsxadmin:{{ fsx_admin_password }}" \
          --env="SCRIPT_B64=${B64}" \
          --overrides='{"spec":{"terminationGracePeriodSeconds":0,"activeDeadlineSeconds":300}}' \
          -- bash -c 'echo ${SCRIPT_B64} | base64 -d | bash'
        RC=$?
        echo ">>> DR unpeer pod exited (rc=${RC})"
      register: _dr_unpeer
      changed_when: true
      failed_when: false
      when: _dr_cluster_lif | default('') != ''

    - name: Display DR unpeer output
      debug:
        var: _dr_unpeer.stdout_lines
      when: _dr_unpeer is not skipped

    - name: Clean up unpeer script
      ansible.builtin.file:
        path: /tmp/ontap-unpeer.sh
        state: absent

    - name: Wait for peering removal to propagate
      pause:
        seconds: 15
      when: (_prod_cluster_lif | default('') != '') or (_dr_cluster_lif | default('') != '')

# -----------------------------------------------------------------------------
- name: Destroy FSx ONTAP Filesystems (parallel)
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip if not destroying
      meta: end_play
      when: not (destroy_resources | default(false))

    # --- Prepare Production FSx destroy (init, plan) ---
    - name: Prepare Production FSx destroy
      include_role:
        name: fsx_ontap_terraform
      vars:
        cluster_info: "{{ prod_cluster }}"
        fsx_label: "Production FSx"
        file_system_name_override: "{{ prod_file_system_name | default('') }}"
        terraform_state_region: "{{ prod_cluster.region }}"
        svm_name: "{{ prod_cluster.cluster_name }}-SVM1"
        allowed_cidrs:
          - "{{ prod_cluster.vpc_cidr }}"
          - "{{ dr_cluster.vpc_cidr }}"
        terraform_destroy: true
        fsx_prepare_only: true

    - name: Save Production FSx destroy prepared state
      set_fact:
        _prod_fsx_prepared: "{{ fsx_prepared }}"

    # --- Prepare DR FSx destroy (init, plan) ---
    - name: Prepare DR FSx destroy
      include_role:
        name: fsx_ontap_terraform
      vars:
        cluster_info: "{{ dr_cluster }}"
        fsx_label: "DR FSx"
        file_system_name_override: "{{ dr_file_system_name | default('') }}"
        terraform_state_region: "{{ prod_cluster.region }}"
        svm_name: "{{ dr_cluster.cluster_name }}-SVM1"
        allowed_cidrs:
          - "{{ dr_cluster.vpc_cidr }}"
          - "{{ prod_cluster.vpc_cidr }}"
        terraform_destroy: true
        fsx_prepare_only: true

    - name: Save DR FSx destroy prepared state
      set_fact:
        _dr_fsx_prepared: "{{ fsx_prepared }}"

    # --- Destroy both in parallel ---
    - name: Display parallel destroy notice
      debug:
        msg:
          - "=========================================="
          - "Destroying FSx ONTAP in PARALLEL"
          - "=========================================="
          - "  Production: {{ _prod_fsx_prepared.file_system_name }}"
          - "  DR:         {{ _dr_fsx_prepared.file_system_name }}"
          - "=========================================="

    - name: Destroy FSx ONTAP - Production (async)
      ansible.builtin.shell: "{{ _prod_fsx_prepared.apply_cmd }}"
      async: 3600
      poll: 0
      register: _prod_fsx_destroy_job
      when: not (_prod_fsx_prepared.skipped | default(false))

    - name: Destroy FSx ONTAP - DR (async)
      ansible.builtin.shell: "{{ _dr_fsx_prepared.apply_cmd }}"
      async: 3600
      poll: 0
      register: _dr_fsx_destroy_job
      when: not (_dr_fsx_prepared.skipped | default(false))

    # --- Wait for both to complete ---
    - name: Wait for Production FSx destroy to complete
      ansible.builtin.async_status:
        jid: "{{ _prod_fsx_destroy_job.ansible_job_id }}"
      register: _prod_fsx_destroy
      until: _prod_fsx_destroy.finished
      retries: 120
      delay: 30
      when: not (_prod_fsx_prepared.skipped | default(false))

    - name: Wait for DR FSx destroy to complete
      ansible.builtin.async_status:
        jid: "{{ _dr_fsx_destroy_job.ansible_job_id }}"
      register: _dr_fsx_destroy
      until: _dr_fsx_destroy.finished
      retries: 120
      delay: 30
      when: not (_dr_fsx_prepared.skipped | default(false))

    - name: Display FSx destroy completion
      debug:
        msg:
          - "=========================================="
          - "FSx ONTAP Filesystems Destroyed"
          - "  Production: {{ _prod_fsx_prepared.file_system_name }}"
          - "  DR:         {{ _dr_fsx_prepared.file_system_name }}"
          - "=========================================="

- name: Destroy VPC Peering
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip if not destroying
      meta: end_play
      when: not (destroy_resources | default(false))

    - name: Destroy VPC peering via Terraform
      include_role:
        name: vpc_peering_terraform
      vars:
        terraform_destroy: true

- name: Destruction Summary
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Skip if not destroying
      meta: end_play
      when: not (destroy_resources | default(false))

    - name: Display destruction summary
      debug:
        msg:
          - "============================================================"
          - "  NetApp DR Infrastructure - Destruction Complete"
          - "============================================================"
          - "  Production FSx: Destroyed"
          - "  DR FSx: Destroyed"
          - "  VPC Peering: Destroyed"
          - "  Terraform State: {{ terraform_state_bucket }} (PRESERVED)"
          - ""
          - "  To also destroy the state backend:"
          - "    make destroy-terraform-state"
          - "============================================================"
